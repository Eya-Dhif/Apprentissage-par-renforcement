{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eya-Dhif/Apprentissage-par-renforcement/blob/main/apprentissage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Awo_UBas5y_F"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.tracebacklimit = 0\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Init Taxi-V2 Env\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# Init arbitrary values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.7 # Momentum 0.2, Current 0.8 Greedy, 0.2 is to reduce volatility and flip flop\n",
        "gamma = 0.2 # Learning Rate 0.1 Greediness is 10%\n",
        "epsilon = 0.4 # explore 10% exploit 90%\n",
        "\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "training_memory = []\n",
        "\n",
        "for i in range(1, 50000):\n",
        "    state = env.reset()\n",
        "\n",
        "    # Init Vars\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    #training\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Check the action space\n",
        "            action = env.action_space.sample() # for explore\n",
        "        else:\n",
        "           # Check the learned values\n",
        "           action = np.argmax(q_table[state]) # for exploit\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) #gym generate, the environment already setup for you\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state]) #take highest from q table for exploit\n",
        "\n",
        "        # Update the new value\n",
        "        new_value = (1 - alpha) * old_value + alpha * \\\n",
        "            (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        # penalty for performance evaluation\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        training_memory.append(q_table.copy())\n",
        "        clear_output(wait=True)\n",
        "        print(\"Episode:\", i)\n",
        "        print(\"Saved q_table during training:\", i)\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7nwiuxG6DXG",
        "outputId": "979ec21b-5e25-47ba-adb1-4bd0740e7309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 49900\n",
            "Saved q_table during training: 49900\n",
            "Training finished.\n",
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [ -1.24999956  -1.24999782  -1.24999956  -1.24999782  -1.24998912\n",
            "  -10.24999782]\n",
            " [ -1.249728    -1.24864     -1.249728    -1.24864     -1.2432\n",
            "  -10.24864   ]\n",
            " ...\n",
            " [ -1.2432      -1.216       -1.2432      -1.24864    -10.2432\n",
            "  -10.2432    ]\n",
            " [ -1.24998912  -1.2499456   -1.24998912  -1.2499456  -10.24998912\n",
            "  -10.24998912]\n",
            " [ -0.4         -1.08        -0.4          3.          -9.4\n",
            "   -9.4       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# At state 499 i will definitely move west\n",
        "state = 499\n",
        "print(training_memory[0][state])\n",
        "print(training_memory[20][state])\n",
        "print(training_memory[50][state])\n",
        "print(training_memory[200][state])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4xMEQDMA-ls",
        "outputId": "da6d312f-7c2c-44cf-cb1b-afcb6a9eb948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.008      -1.07999534 -1.008       2.6481     -7.         -7.        ]\n",
            "[-0.45472    -1.08       -0.40001197  3.         -9.39984286 -9.39947512]\n",
            "[-0.4  -1.08 -0.4   3.   -9.4  -9.4 ]\n",
            "[-0.4  -1.08 -0.4   3.   -9.4  -9.4 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# At state 77 i will definitely move east\n",
        "state = 77\n",
        "print(training_memory[0][state])\n",
        "print(training_memory[20][state])\n",
        "print(training_memory[50][state])\n",
        "print(training_memory[200][state])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjicAXXDBB9C",
        "outputId": "55456dfa-ba5f-469a-d3c9-c6d26c72dc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.071      -0.65967246  2.99044327 -1.14352    -8.6948658  -7.        ]\n",
            "[-1.08       -0.4         3.         -1.08       -9.39984579 -9.39999997]\n",
            "[-1.08 -0.4   3.   -1.08 -9.4  -9.4 ]\n",
            "[-1.08 -0.4   3.   -1.08 -9.4  -9.4 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To show that at state 393, how the move evolved\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "SFfLM2thBPM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_dict = {0: \"move south\"\n",
        ",1: \"move north\"\n",
        ",2: \"move east\"\n",
        ",3: \"move west\"\n",
        ",4: \"pickup passenger\"\n",
        ",5: \"dropoff passenger\"\n",
        "}\n",
        "\n",
        "ENV_STATE = env.reset()\n",
        "print(env.render(mode='ansi'))\n",
        "state_memory = [i[ENV_STATE] for i in training_memory]\n",
        "printmd(\"For state **{}**\".format(ENV_STATE))\n",
        "for step, i in enumerate(state_memory):\n",
        "\n",
        "    if step % 200==0:\n",
        "        choice = np.argmax(i)\n",
        "        printmd(\"for episode in {}, q table action is {} and it will ... **{}**\".format(step*100, choice, action_dict[choice]))\n",
        "        print(i)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "iht1SQAiBkpt",
        "outputId": "6f10675d-a2f2-47ca-8a81-4c615e66a0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y|\u001b[43m \u001b[0m: |B: |\n",
            "+---------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "For state **424**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "for episode in 0, q table action is 3 and it will ... **move west**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.24997561  -1.24999297  -1.24997129  -1.24996925 -10.24727866\n",
            " -10.15480742]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "for episode in 20000, q table action is 1 and it will ... **move north**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.25  -1.25  -1.25  -1.25 -10.25 -10.25]\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "for episode in 40000, q table action is 1 and it will ... **move north**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -1.25  -1.25  -1.25  -1.25 -10.25 -10.25]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episode: {frame['episode']}\")\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        time.sleep(0.8)\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 10 # Try 10 rounds\n",
        "frames = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state]) # deterministic (exploit), not stochastic (explore), only explore in training\n",
        "        env\n",
        "        state, reward, done, info = env.step(action) #gym\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        # Put each rendered frame into dict for animation, gym generated\n",
        "        frames.append({\n",
        "           'frame': env.render(mode='ansi'),\n",
        "           'episode': ep,\n",
        "           'state': state,\n",
        "           'action': action,\n",
        "           'reward': reward\n",
        "            }\n",
        "        )\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print_frames(frames)\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOtgZJR6CnRN",
        "outputId": "4627f1c9-c161-446a-8ef2-262edb588050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episode: 9\n",
            "Timestep: 123\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n",
            "Results after 10 episodes:\n",
            "Average timesteps per episode: 12.3\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    }
  ]
}